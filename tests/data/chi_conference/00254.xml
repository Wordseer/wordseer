<article>
<year>2010</year>
<title>Crowdsourcing graphical perception: using mechanical turk to assess visualization design</title>
<author>Heer, Jeffrey</author>
<author>Bostock, Michael</author>
<keyword>crowdsourcing</keyword>
<keyword>evaluation</keyword>
<keyword>experimentation</keyword>
<keyword>graphical perception</keyword>
<keyword>information visualization</keyword>
<keyword>mechanical turk</keyword>
<keyword>user study</keyword>
<abstract>Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.</abstract>
</article>